{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1_xlKNkEXX10IsPJ3LyH4jPef6i1RmW2V",
      "authorship_tag": "ABX9TyMSUCL/rH1WML7BdN7Onc/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/don05050505/don05050505/blob/main/1hotincoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksio2hso5J2V"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "8J3XF82v5W-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all', quiet=True)"
      ],
      "metadata": {
        "id": "-PGouWAx5Zw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Natural language processing (NLP) 한글 is a subfield of \\\n",
        "linguistics, computer science, and artificial intelligence \\\n",
        "concerned with the interactions between computers and human language, \\\n",
        "in particular how to program computers to process and analyze large \\\n",
        "amounts of natural language data. 한글 The goal is a computer capable of \\\n",
        "understanding the contents of documents, including the contextual\"\n",
        "# tokens = word_tokenize(text)\n",
        "sentence = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "3un-3VUW5umD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence)"
      ],
      "metadata": {
        "id": "m2z4r_Wu6qdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "bVikNwpR8Mui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
        "\n",
        "uniqueWord = array(set(data))\n",
        "vectors = array(data)\n",
        "print(vectors)"
      ],
      "metadata": {
        "id": "KR_l5DxC87U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "int_encoded = label_encoder.fit_transform(vectors)\n",
        "print(int_encoded)"
      ],
      "metadata": {
        "id": "0NlW1lWz9Pep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OneHotEncoder = OneHotEncoder(sparse=False)\n",
        "int_encoded = int_encoded.reshape(len(int_encoded), 1)\n",
        "print(int_encoded)"
      ],
      "metadata": {
        "id": "ZyHjZuQQ-8sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming int_encoded is a 1D array or a column vector\n",
        "int_encoded = int_encoded.reshape(-1, 1)\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform int_encoded\n",
        "onehot_encoded = onehot_encoder.fit_transform(int_encoded)\n",
        "\n",
        "# Print the one-hot encoded result\n",
        "print(onehot_encoded)\n"
      ],
      "metadata": {
        "id": "L6KeALvvBFko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[9, :])])\n",
        "print(inverted)"
      ],
      "metadata": {
        "id": "CDfdAaSyBK4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cold:0\n",
        "hot:1\n",
        "warm:2\n",
        "arrsCold = [0,0,0]\n",
        "arrsCold[0] = 1\n",
        "arrsHot = [0,0,0]\n",
        "arrsHot[1] = 1\n",
        "arrsWarm = [0,0,0]\n",
        "arrsWarm[2] = 1\n",
        "dicts = {'cold':[1,0,0], 'hot':[0,1,0], 'warm': [0,0,1]}\n"
      ],
      "metadata": {
        "id": "dMX5aCuKCMmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I=[0,0,1]\n",
        "onehot = array(I)\n",
        "idx = argmax(onehot)\n",
        "dicts = {'cold':[1,0,0], 'hot':[0,1,0], 'warm': [0,0,1]}\n",
        "label = argmax(dicts['cold'])\n",
        "print(label)"
      ],
      "metadata": {
        "id": "bYZ-TBSEDrLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}