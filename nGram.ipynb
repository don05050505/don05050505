{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1_xlKNkEXX10IsPJ3LyH4jPef6i1RmW2V",
      "authorship_tag": "ABX9TyPPNxRR59NdWe9o4WFkn1ax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/don05050505/don05050505/blob/main/nGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "metadata": {
        "id": "WafjXY_8Ep5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten"
      ],
      "metadata": {
        "id": "J0vJNvMqE8VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all', quiet=True)"
      ],
      "metadata": {
        "id": "HJN-ZOtPFfKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlp"
      ],
      "metadata": {
        "id": "Fz1CVbCEF3Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [['i', 'like', 'eat'], ['i', 'like', 'eat']]"
      ],
      "metadata": {
        "id": "9ldmIjxAGLcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "big = list(bigrams(text[1]))\n",
        "print(big)"
      ],
      "metadata": {
        "id": "LAOJtJQaGYCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"if I were you, I would wprk out now.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tokens.remove('.')\n",
        "tokens.remove(',')\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "OZ68UgWKHa4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big = list(bigrams(tokens))\n",
        "print(big)"
      ],
      "metadata": {
        "id": "GY5qAl1zIHZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram = list(ngrams(tokens, n=3))\n",
        "print(ngram)"
      ],
      "metadata": {
        "id": "nu4Jqeb6Iaeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = ''' 인공지능(AI)이 산업을 넘어 일상속으로 점점 더 파고들고 있다. AI가 그림을 그리고 작곡을 하고 시를 짓는 건 더 이상 '쇼킹'한 일이 아니다. 개인도 기업도 국가도 AI활용에 뒤지면 경쟁력에 뒤지는 시대가 왔다. 우리 정부가 내년에 1조원 가까운 예산을 들여 의료, 교육, 문화, 예술, 교육, 보건 등 국민이 체감할 수 있는 분야 전반에 인공지능(AI) 기술을 적용하는 이유이기도 하다.\n",
        "\n",
        "오는 25~27일 서울 코엑스 1층에서 열리는 '2023 대한민국디지털기술혁신대전(2023 디미혁)'은 일상에 스며든 AI를 체감할 수 있는 장(場)이다.\n",
        "\n",
        "특히 LG AI연구원(원장 배경훈)이 마련한 전시 부스에 가면 이 회사가 지난 7월 서울 강서구 마곡 LG사이언스파크 컨버전스홀에서 열린 'LG AI 토크 콘서트’에서 공개한 초거대AI '엑사원(EXAONE) 2.0'을 직접 체험하며 AI의 '놀라운 힘'을 경험할 수 있다. LG AI연구원이 다른 계열사없이 독자적으로 전시 부스를 마련한 건 이번 '2023 디미혁' 행사가 처음이다.\n",
        "\n",
        "'엑사원 2.0'은 크게 ▲아틀리에(Atelier) ▲유니버스(Universe) ▲디스커버리(Discovery) 등 3대 플랫폼으로 구성됐다. 이중 '아틀리에(Atelier)'는 텍스트와 텍스트에 맞는 배경 이미지를 손쉽게 만들어주는 멀티모달 AI플랫폼으로, 누구나 동화책을 만들수 있게 해준다. 인간의 창의적 발상을 돕는 AI인것이다. 이미지를 언어로 표현하고 언어를 이미지로 시각화할 수 있다. LG의 초거대 AI ‘엑사원’과 디자이너가 작업하는 공간을 의미하는 단어인 ‘아틀리에’를 합쳐 만들었다.\n",
        "\n",
        "\n",
        "배경훈 LG AI연구원장이 지난 7월 서울 강서구 마곡동 LG사이언스파크에서 열린 LG AI 토크콘서트에서 AI 플랫폼 '엑사원 2.0'(EXAONE 2.0)을 소개하고 있다\n",
        " LG는 최근 세계 3대 디자인 스쿨 중 하나인 ‘파슨스(Parsons School of Design)’와 함께 AI로 세상에 없던 디자인을 창조하기 위한 서비스를 개발한다고 밝힌 바 있는데, 여기에 '아틀리에'가 적용된다. '아틀리에'를 사용해 초거대 AI와 인간 디자이너가 협업, 세상에 없는 창조적 작품을 만든다는 것이다.\n",
        "\n",
        "\n",
        "김유철 LG AI연구원 부문장은 '엑사원 아틀리에’에 대해 \"새롭고 참신한 이미지를 찾는데 목말라 있는 디자이너들이 자신의 머릿속에서 맴돌고 있는 아이디어를 현실에서 시각적인 이미지로 구현하는 작업에 많은 시간을 쏟고 있다는 점에 착안해 개발한 플랫폼\"이라면서 \" LG AI연구원은 파슨스와 ‘엑사원 아틀리에’로 전문 디자이너의 아이디어를 시각화하는 것을 넘어 새로운 디자인 방법론을 개발하기 위한 중장기 공동 연구를 진행한다\"고 밝혔다.\n",
        "\n",
        "‘엑사원 아틀리에’의 두뇌에 해당하는 초거대AI ‘엑사원’은 텍스트와 결합된 고해상도 이미지 3억 5천만 장 이상 데이터를 학습해 언어 맥락까지 이해할 뿐 아니라 기존에 없던 새로운 이미지를 창작(Text to Image)하는 능력을 갖췄다. 하나의 문장만으로 7분 만에 256장의 고해상도 이미지를 생성할 수 있다고 LG는 말한다. 디자이너가 사진과 그림, 음성과 영상 등 일상에서 마주하는 다양한 형태의 정보들을 경험한 느낌과 생각을 플랫폼에 기록해 놓으면 ‘엑사원’이 이를 학습할 수 있게 했다.\n",
        "\n",
        "\n",
        "LG AI연구원은 최근 세계적 디자인 학교인 파슨스와 협력하기로 했다. 왼쪽 다섯번째가 배경훈 원장.\n",
        "김승환 LG AI연구원 비전랩장은 “디자이너가 엑사원과 함께 세상에 없던 이미지를 만드는 작업을 반복하며, 자신만의 창의적인 디자인 컨셉을 구축할 수 있을 것”이라며 “AI와 인간의 협업이 단순히 신기한 이미지를 생성한다는 의미를 넘어 디자이너의 창의력이 어디까지 발전할 수 있을지 가능성을 확인하고 작품 활동을 하는 데 실질적으로 기여하고자 한다”고 말했다.\n",
        "\n",
        "전문가용 대화형 AI플랫폼인 '엑사원 유니버스'도 시선을 모을 전망이다. '유니버스'는 전문성을 요하는 분야의 질문에 근거에 기반한 답변을 생성하는 AI 플랫폼이다. 화학, 바이오, 제약, 의료, 금융, 특허 등 도메인 별 특화 서비스를 구축중이다.\n",
        "\n",
        "관련기사\n",
        "국가전략기술 양자 컴퓨터, 눈으로 확인하세요2023.09.14\n",
        "'로봇계 다빈치' 데니스 홍, 26일 코엑스 온다2023.09.13\n",
        "디지털 미래 한 눈에 보여줄 '혁신대전' 열린다2023.09.11\n",
        "\"국내 유일 백엔드 특화 노코드 툴로 디지털 전환 가속\"2023.07.28\n",
        "또 '엑사원 디스커버리'는 화학 및 바이오 분야 발전을 앞당길 신소재·신물질·신약 개발 플랫폼이다. 멀티모달 AI 기술을 활용해 전문 문헌 텍스트 뿐 아니라 분자 구조, 수식, 차트, 테이블, 이미지 등 비(非)텍스트 정보까지 데이터베이스화했다. AI와 대화하며 ▲전문 문헌 검토 ▲소재 구조 설계 ▲소재 합성 예측이 가능하다. 특히 1만회가 넘었던 합성 시행착오를 수십회로 줄이고 연구개발 소요 시간도 40개월에서 5개월로 단축할 것으로 예상됐다.\n",
        "\n",
        "'엑사원 2.0' 공개 당시 배경훈 LG AI연구원장은 \"LG는 국내에서 유일하게 이중 언어 모델과 양방향 멀티모달 모델을 모두 상용화한 기업이며, 세상의 지식을 이해하고 발견하는 상위 1%의 전문가 AI를 개발하고 있다”면서 “’다른 생성형 AI들과는 차별화된 고객 가치’를 창출하는 글로벌 경쟁력을 갖춘 AI 컴퍼니로 발전해 나갈 것”이라고 강조했다.   '''"
      ],
      "metadata": {
        "id": "KVqldn7eQYgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(news)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "6ycqeGESJeOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram = list(ngrams(tokens, n=4))\n",
        "print(ngram)"
      ],
      "metadata": {
        "id": "SfBef2XVJztK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_list = []\n",
        "\n",
        "search_word = input(\"찾고자 하는 단어를 입력하세요: \")\n",
        "\n",
        "for looking in ngram:\n",
        "    if search_word in looking:\n",
        "        ngram_list.append(looking)\n",
        "\n",
        "print(\"찾은 결과:\")\n",
        "print(ngram_list)\n"
      ],
      "metadata": {
        "id": "aXWy1IvQScNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"if I were you, I would wprk out now.\""
      ],
      "metadata": {
        "id": "vv194Y0gUbxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import pad_sequence\n",
        "sentence = sentence.replace(',', ' ')\n",
        "characters = pad_sequence(sentence,\n",
        "                         pad_left=True, left_pad_symbol='<s>',\n",
        "                         pad_right=True, right_pad_symbol='</s>',\n",
        "                         n=2)\n",
        "characters = list(characters)\n",
        "\n",
        "\n",
        "# print(characters)\n",
        "ngramChar = list(ngrams(characters, n=4))\n",
        "print(ngramChar)"
      ],
      "metadata": {
        "id": "qhcT7t3FUq--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "contTokens = sent_tokenize(news)\n",
        "print(len(contTokens))\n",
        "# print(contTokens)\n",
        "# print(len(contTokens))\n",
        "# ngramTokens =[]\n",
        "# for content in  contTokens:\n",
        "#     token = word_tokenize(content)\n",
        "#     token.insert(0, '<s>')\n",
        "#     token.insert(-1, '</s>')\n",
        "#     ngramTokens = ngramTokens + list(ngrams(token, n=4))\n",
        "# print(ngramTokens)\n",
        "\n",
        "ngramSent =''\n",
        "for content in  contTokens:\n",
        "    token = word_tokenize(content)\n",
        "    tokens = list(pad_sequence(token,\n",
        "                               pad_left=True, left_pad_symbol='start',\n",
        "                               pad_right=True, right_pad_symbol='end',\n",
        "                               n=2))\n",
        "    ngramSent = ngramSent + ' ' + ' '.join(tokens)\n",
        "\n",
        "wordTokens = word_tokenize(ngramSent)\n",
        "\n",
        "\n",
        "ngramTokens = list(ngrams(wordTokens, n=5))\n",
        "\n",
        "print(wordTokens)"
      ],
      "metadata": {
        "id": "cHwbPkteZRrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}